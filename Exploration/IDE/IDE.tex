%\section{Information-Driven Exploration}
\iffalse
Exploration is a Partially-Observed Markov Decision Process (POMDP).
$$\min_{\text{Policies $\pi$}}\sum $$
policy with the shortest expected \footnote{or shortest worst-case}
\begin{example}
Lost Astronaut:
An astronaut crash-lands at an unknown location on the Earth's surface, 
having only a prior distribution on latitude, derived from the trajectory of
her satellite (Fig. \ref{fig: lost astronaut}.
Needing to know her exact location, and having broken her navigation equipment, 
she examines local landmarks.
Her prior knowledge of the situation informs her initial reconnaisance.  
For instance, as her longitude is more uncertain than her latitude,
it would be a better use of her time to try to ascertain the former, by, 
for instance, checking the passage of the sun against her watch.
\fi


